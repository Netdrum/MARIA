{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOK8Ak6VEuBH5oWlruCYpNm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Netdrum/MARIA/blob/main/20241119_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"
      ],
      "metadata": {
        "id": "3eLNQKLlyjxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import WhisperFeatureExtractor, WhisperProcessor, WhisperTokenizer, WhisperForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import torchaudio\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# Load WER metric\n",
        "wer_metric = load(\"wer\")\n",
        "\n",
        "# Define global constants\n",
        "MAX_AUDIO_FRAMES = 3000  # Max length for spectrograms (adjust based on GPU memory)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Netdrum/IRCG_VHF\")  # Replace with your dataset name\n",
        "\n",
        "# Split dataset into train/test if not already split\n",
        "if \"train\" not in dataset or \"test\" not in dataset:\n",
        "    dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Initialize Whisper components\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data(batch):\n",
        "    try:\n",
        "        audio = batch[\"audio\"]\n",
        "        waveform = torch.tensor(audio[\"array\"], dtype=torch.float32)\n",
        "        sampling_rate = audio[\"sampling_rate\"]\n",
        "\n",
        "        # Resample if necessary\n",
        "        if sampling_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Convert audio to log-Mel spectrogram\n",
        "        spectrogram_transform = MelSpectrogram(sample_rate=16000, n_mels=80)\n",
        "        spectrogram = spectrogram_transform(waveform).numpy()\n",
        "\n",
        "        # Truncate or pad spectrogram to MAX_AUDIO_FRAMES\n",
        "        if spectrogram.shape[1] > MAX_AUDIO_FRAMES:\n",
        "            spectrogram = spectrogram[:, :MAX_AUDIO_FRAMES]\n",
        "        else:\n",
        "            pad_width = MAX_AUDIO_FRAMES - spectrogram.shape[1]\n",
        "            spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
        "\n",
        "        # Tokenize text\n",
        "        labels = tokenizer(batch[\"transcription\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256).input_ids\n",
        "\n",
        "        return {\n",
        "            \"input_features\": spectrogram,\n",
        "            \"labels\": labels.squeeze(0).tolist(),  # Convert to list for easier processing\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing entry: {e}\")\n",
        "        return None\n",
        "\n",
        "# Apply preprocessing and filter invalid samples\n",
        "dataset = dataset.map(preprocess_data, remove_columns=[\"audio\", \"transcription\"], batched=False)\n",
        "dataset = dataset.filter(lambda x: x is not None)\n",
        "\n",
        "# Define custom data collator\n",
        "class WhisperDataCollator:\n",
        "    def __call__(self, features):\n",
        "        # Extract input features\n",
        "        input_features = torch.stack([torch.tensor(f[\"input_features\"], dtype=torch.float32) for f in features])\n",
        "\n",
        "        # Extract and pad labels\n",
        "        max_label_length = max(len(f[\"labels\"]) for f in features)\n",
        "        padded_labels = torch.stack([\n",
        "            torch.nn.functional.pad(\n",
        "                torch.tensor(f[\"labels\"], dtype=torch.long),\n",
        "                (0, max_label_length - len(f[\"labels\"])),\n",
        "                value=-100  # Padding value for cross-entropy loss\n",
        "            )\n",
        "            for f in features\n",
        "        ])\n",
        "\n",
        "        return {\"input_features\": input_features, \"labels\": padded_labels}\n",
        "\n",
        "# Split dataset into train/test\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-3,#changed down from 5\n",
        "    per_device_train_batch_size=16,#changed this from 8 to 16\n",
        "    per_device_eval_batch_size=4,# changed this from 8 to 4\n",
        "    num_train_epochs=5,#changed this to 5 but model still only running 3\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True,  # Use mixed precision training if supported by GPU\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    logging_steps=10,  # Log training loss every 10 steps\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    output_dir=\"./results\",\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=WhisperDataCollator(),\n",
        "    tokenizer=processor,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model():\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for batch in test_dataset:\n",
        "        # Convert input features to tensor and ensure type matches model (float16 if fp16 training)\n",
        "        inputs = torch.tensor(batch[\"input_features\"], dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "        if training_args.fp16:\n",
        "            inputs = inputs.half()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(inputs)\n",
        "\n",
        "        decoded_preds = processor.batch_decode(outputs, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode([batch[\"labels\"]], skip_special_tokens=True)\n",
        "\n",
        "        predictions.extend(decoded_preds)\n",
        "        references.extend(decoded_labels)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
        "    print(f\"Word Error Rate (WER): {wer}\")\n"
      ],
      "metadata": {
        "id": "7bY7pjTBjAM0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}