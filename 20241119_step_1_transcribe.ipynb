{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11mILjjlQbJIgC6SydkNQK4_or4L-hLpu",
      "authorship_tag": "ABX9TyO0HeBLN9/ix0nDYYaru98S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Netdrum/MARIA/blob/main/20241119_step_1_transcribe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kyPz0D5kwsC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install datasets==2.12.0"
      ],
      "metadata": {
        "id": "7ZqFFulxx_uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('IRCG_VHF')"
      ],
      "metadata": {
        "id": "PlNijCRMxjLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import re  # Optional for filename parsing (if speaker ID needed)\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import torch  # Import torch explicitly\n",
        "import librosa  # Import librosa for audio loading\n",
        "\n",
        "def prepare_VHF_data(data_folder=\"/content/drive/MyDrive/Dataset\", output_dir=\"manifest_output\"):\n",
        "  \"\"\"\n",
        "  This function processes VHF audio files in a folder using Whisper for transcription and generates:\n",
        "      - JSON manifest files for each audio file with transcription and audio information.\n",
        "      - A single CSV file summarizing the processed audio data.\n",
        "\n",
        "  Args:\n",
        "      data_folder: Path to the folder containing VHF audio files (default: \"/content/drive/MyDrive/Sample Data Set\").\n",
        "      output_dir: Path to the directory where manifest files and the CSV will be saved (default: \"manifest_output\").\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the output directory if it doesn't exist\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  # Load Whisper model and processor\n",
        "  model_name = \"openai/whisper-small\"\n",
        "  processor = WhisperProcessor.from_pretrained(model_name)\n",
        "  model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "  try:\n",
        "      print(\"Whisper model loaded successfully.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error loading Whisper model: {e}\")\n",
        "      return\n",
        "\n",
        "  # Create an empty list to store CSV data\n",
        "  csv_data = []\n",
        "\n",
        "  for filename in os.listdir(data_folder):\n",
        "    if filename.endswith(\".wav\"):  # Check for audio file extension\n",
        "\n",
        "      # Print file paths for verification\n",
        "      print(f\"Processing file: {filename}\")\n",
        "      filepath = os.path.join(data_folder, filename)\n",
        "\n",
        "      # Load audio data using librosa and retrieve sampling rate (if available)\n",
        "      try:\n",
        "          audio_data, sample_rate = librosa.load(filepath, sr=None)  # Load with native sample rate\n",
        "      except Exception as e:\n",
        "          print(f\"Error loading audio file: {filename} ({e})\")\n",
        "          continue  # Skip to the next file if loading fails\n",
        "\n",
        "      # Transcribe audio using Whisper (passing the sampling rate)\n",
        "      inputs = processor(audio_data, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
        "      with torch.no_grad():\n",
        "          outputs = model.generate(**inputs)\n",
        "      transcription = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "      # Extract speaker ID from filename (optional, adjust regex if needed)\n",
        "      speaker_id = None  # Replace with appropriate regex if you want speaker ID\n",
        "      # You can uncomment and modify the following line for speaker ID extraction:\n",
        "      # match = re.search(r\"speaker_(\\d+)\", filename)  # Example regex for \"speaker_XX.wav\" format\n",
        "      # if match:\n",
        "      #     speaker_id = match.group(1)\n",
        "\n",
        "      # Create dictionary entry with desired format\n",
        "      entry = {\n",
        "          \"wav\": os.path.join(\"{data_root}\", filename),  # Placeholder for data_root\n",
        "          \"length\": len(audio_data) / sample_rate,  # Duration\n",
        "          \"spk_id\": speaker_id,  # Optional speaker ID from filename\n",
        "          \"ali\": None,  # Phoneme alignment information (if you have it)\n",
        "          \"phn\": None,  # Phoneme information (if you have it)\n",
        "          \"text\": transcription  # Text information\n",
        "      }\n",
        "\n",
        "      if sample_rate is not None:\n",
        "          entry[\"frame_rate\"] = sample_rate  # Add frame rate if retrieved\n",
        "\n",
        "        # Split into train and test sets\n",
        "    all_entries = []  # Collect all entries for dataset creation\n",
        "    for filename in os.listdir(data_folder):\n",
        "        if filename.endswith(\".wav\"):\n",
        "\n",
        "            # Append the entry to all_entries\n",
        "            all_entries.append(entry)\n",
        "\n",
        "    # Create a Hugging Face Dataset from the entries\n",
        "    dataset = Dataset.from_list(all_entries)\n",
        "\n",
        "    # Split the dataset into train and test\n",
        "    dataset = dataset.train_test_split(test_size=test_size)\n",
        "\n",
        "    # Rename the splits to \"train\" and \"test\"\n",
        "    dataset = DatasetDict({\"train\": dataset[\"train\"], \"test\": dataset[\"test\"]})\n",
        "\n",
        "    # Upload to Hugging Face\n",
        "    dataset.push_to_hub(\"IRCG_VHF\")  # Replace with your details\n",
        "\n",
        "      # Generate output filename (replace \".wav\" with \".json\")\n",
        "    output_filename = os.path.splitext(filename)[0] + \".json\"  # Create JSON filename from audio filename\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "      # Write entry to a JSON file for each audio file\n",
        "    with open(output_path, \"w\") as f:\n",
        "          json.dump(entry, f, indent=4)\n",
        "\n",
        "      # Create a CSV row for each audio file and append to csv_data\n",
        "    csv_row = {\n",
        "          \"filename\": filename,\n",
        "          \"duration\": entry[\"length\"],\n",
        "          \"speaker_id\": entry[\"spk_id\"] if entry[\"spk_id\"] else None,  # Include speaker ID if it exists\n",
        "      } # Add this closing curly brace"
      ],
      "metadata": {
        "id": "6CTQT30FaQPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}